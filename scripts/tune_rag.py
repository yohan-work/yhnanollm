#!/usr/bin/env python3
"""
RAG íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì‹¤í—˜ ë„êµ¬
ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ìµœì  ì„¤ì •ì„ ì°¾ìŠµë‹ˆë‹¤
"""

import argparse
import json
import csv
from pathlib import Path
from datetime import datetime
from itertools import product
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from chat import LocalLLMChat
from rag import DocumentProcessor, VectorStore, RAGChain, DocumentManager
from rag.config import RAGConfig
from rag.prompts import list_templates


class RAGTuner:
    """RAG íŒŒë¼ë¯¸í„° íŠœë‹ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        model_path: str = "mlx-community/Llama-3.2-1B-Instruct-4bit",
        adapter_path: str = "models/lora-adapter",
        test_data_path: str = None
    ):
        """
        Args:
            model_path: LLM ëª¨ë¸ ê²½ë¡œ
            adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ
            test_data_path: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ JSON íŒŒì¼ ê²½ë¡œ
        """
        self.model_path = model_path
        self.adapter_path = adapter_path if Path(adapter_path).exists() else None
        self.test_data_path = test_data_path
        
        # LLM ì´ˆê¸°í™”
        print("ğŸ”„ LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.llm_chat = LocalLLMChat(
            model_path=self.model_path,
            adapter_path=self.adapter_path,
            max_tokens=150
        )
        self.llm_chat.load_model()
        print("âœ… LLM ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ
        self.test_questions = self._load_test_questions()
    
    def _load_test_questions(self):
        """í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ ë¡œë“œ"""
        if self.test_data_path and Path(self.test_data_path).exists():
            with open(self.test_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('questions', [])
        
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        return [
            {
                "question": "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "category": "summary"
            },
            {
                "question": "ì‹ ì²­ ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "category": "process"
            },
            {
                "question": "ì‹ ì²­ ìê²© ìš”ê±´ì´ ë¬´ì—‡ì¸ê°€ìš”?",
                "category": "requirements"
            },
            {
                "question": "ì§€ì› ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
                "category": "specific"
            },
            {
                "question": "ë¬¸ì˜ì²˜ëŠ” ì–´ë””ì¸ê°€ìš”?",
                "category": "contact"
            }
        ]
    
    def run_experiment(
        self,
        chunk_sizes: list = [300, 500, 800],
        chunk_overlaps: list = [50, 100],
        top_ks: list = [1, 3, 5],
        prompt_templates: list = ["default", "detailed", "korean_optimized"],
        output_dir: str = "experiments"
    ):
        """
        íŒŒë¼ë¯¸í„° ì¡°í•©ë³„ ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            chunk_sizes: í…ŒìŠ¤íŠ¸í•  ì²­í¬ í¬ê¸° ë¦¬ìŠ¤íŠ¸
            chunk_overlaps: í…ŒìŠ¤íŠ¸í•  ì²­í¬ ì˜¤ë²„ë© ë¦¬ìŠ¤íŠ¸
            top_ks: í…ŒìŠ¤íŠ¸í•  top-k ê°’ ë¦¬ìŠ¤íŠ¸
            prompt_templates: í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¦¬ìŠ¤íŠ¸
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = output_path / f"tuning_results_{timestamp}.csv"
        
        # ê²°ê³¼ ì €ì¥ìš© CSV í—¤ë”
        headers = [
            'chunk_size', 'chunk_overlap', 'top_k', 'prompt_template',
            'avg_answer_length', 'documents_used', 'avg_distance',
            'total_time', 'config_score'
        ]
        
        results = []
        total_combinations = len(chunk_sizes) * len(chunk_overlaps) * len(top_ks) * len(prompt_templates)
        current = 0
        
        print(f"\nğŸ”¬ ì´ {total_combinations}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìˆ˜: {len(self.test_questions)}")
        print("="*60)
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸
        for chunk_size, chunk_overlap, top_k, prompt_template in product(
            chunk_sizes, chunk_overlaps, top_ks, prompt_templates
        ):
            current += 1
            print(f"\n[{current}/{total_combinations}] í…ŒìŠ¤íŠ¸ ì¤‘...")
            print(f"  chunk_size={chunk_size}, overlap={chunk_overlap}, "
                  f"top_k={top_k}, prompt={prompt_template}")
            
            try:
                # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                config = RAGConfig(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    top_k=top_k,
                    prompt_template=prompt_template
                )
                
                result = self._test_configuration(config)
                result.update({
                    'chunk_size': chunk_size,
                    'chunk_overlap': chunk_overlap,
                    'top_k': top_k,
                    'prompt_template': prompt_template
                })
                
                results.append(result)
                
                print(f"  âœ… ì™„ë£Œ - ì ìˆ˜: {result['config_score']:.2f}")
            
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        with open(results_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(results)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")
        
        # ìµœì  ì„¤ì • ë¶„ì„ ë° ì¶”ì²œ
        self._analyze_results(results, output_path / f"analysis_{timestamp}.json")
        
        return results
    
    def _test_configuration(self, config: RAGConfig):
        """
        íŠ¹ì • ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        Args:
            config: RAGConfig ì¸ìŠ¤í„´ìŠ¤
            
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import time
        
        # ì„ì‹œ ë²¡í„° DB ìƒì„± (ì‹¤ì œ ë¬¸ì„œê°€ ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        vector_store = VectorStore(persist_directory=config.persist_directory)
        doc_manager = DocumentManager(metadata_path=config.metadata_path)
        
        # RAG ì²´ì¸ ìƒì„±
        rag_chain = RAGChain(
            vector_store=vector_store,
            llm_chat=self.llm_chat,
            document_manager=doc_manager,
            top_k=config.top_k,
            prompt_template=config.prompt_template,
            similarity_threshold=config.similarity_threshold
        )
        
        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê²½ê³ 
        if vector_store.get_document_count() == 0:
            print("  âš ï¸ ë²¡í„° DBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return {
                'avg_answer_length': 0,
                'documents_used': 0,
                'avg_distance': 0,
                'total_time': 0,
                'config_score': 0
            }
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì— ëŒ€í•´ ì‹¤í–‰
        answer_lengths = []
        documents_counts = []
        distances = []
        
        start_time = time.time()
        
        for test_item in self.test_questions:
            question = test_item['question']
            
            try:
                answer, metadatas, stats = rag_chain.answer(question, use_rag=True)
                
                answer_lengths.append(len(answer))
                
                if stats:
                    documents_counts.append(stats.get('documents_found', 0))
                    distances.append(stats.get('avg_distance', 0))
            
            except Exception as e:
                print(f"    ì§ˆë¬¸ ì‹¤íŒ¨: {question[:30]}... - {str(e)}")
                continue
        
        total_time = time.time() - start_time
        
        # í‰ê·  ê³„ì‚°
        avg_answer_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0
        avg_documents_used = sum(documents_counts) / len(documents_counts) if documents_counts else 0
        avg_distance = sum(distances) / len(distances) if distances else 0
        
        # ì„¤ì • ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        config_score = self._calculate_score(
            avg_answer_length,
            avg_documents_used,
            avg_distance,
            total_time
        )
        
        return {
            'avg_answer_length': round(avg_answer_length, 2),
            'documents_used': round(avg_documents_used, 2),
            'avg_distance': round(avg_distance, 4),
            'total_time': round(total_time, 2),
            'config_score': round(config_score, 2)
        }
    
    def _calculate_score(self, answer_length, documents_used, avg_distance, total_time):
        """
        ì„¤ì • ì ìˆ˜ ê³„ì‚°
        
        íœ´ë¦¬ìŠ¤í‹±:
        - ë‹µë³€ ê¸¸ì´ê°€ ì ì ˆí•œì§€ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ì§€)
        - ë¬¸ì„œ í™œìš©ë„ (ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜)
        - ê²€ìƒ‰ ì •í™•ë„ (í‰ê·  ê±°ë¦¬ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        - ì†ë„ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ)
        """
        # ë‹µë³€ ê¸¸ì´ ì ìˆ˜ (100~500ìê°€ ì´ìƒì )
        length_score = 10.0
        if 100 <= answer_length <= 500:
            length_score = 10.0
        elif answer_length < 100:
            length_score = answer_length / 10
        else:
            length_score = max(0, 10 - (answer_length - 500) / 100)
        
        # ë¬¸ì„œ í™œìš© ì ìˆ˜
        doc_score = min(10.0, documents_used * 3)
        
        # ê²€ìƒ‰ ì •í™•ë„ ì ìˆ˜ (ê±°ë¦¬ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        distance_score = max(0, 10 - avg_distance * 10)
        
        # ì†ë„ ì ìˆ˜ (5ì´ˆ ì´ë‚´ê°€ ì´ìƒì )
        speed_score = max(0, 10 - total_time / 5)
        
        # ê°€ì¤‘ í‰ê· 
        total_score = (
            length_score * 0.3 +
            doc_score * 0.3 +
            distance_score * 0.3 +
            speed_score * 0.1
        )
        
        return total_score
    
    def _analyze_results(self, results, output_path):
        """ê²°ê³¼ ë¶„ì„ ë° ìµœì  ì„¤ì • ì¶”ì²œ"""
        if not results:
            print("âš ï¸ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì ìˆ˜ë³„ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x['config_score'], reverse=True)
        
        # ìƒìœ„ 3ê°œ ì„¤ì •
        top_3 = sorted_results[:3]
        
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'total_configurations': len(results),
            'top_configurations': top_3,
            'recommendations': self._generate_recommendations(top_3, results)
        }
        
        # JSONìœ¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        print("\n" + "="*60)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼")
        print("="*60)
        print(f"\nğŸ† ìµœê³  ì ìˆ˜ ì„¤ì •:")
        best = top_3[0]
        print(f"  - ì ìˆ˜: {best['config_score']:.2f}")
        print(f"  - chunk_size: {best['chunk_size']}")
        print(f"  - chunk_overlap: {best['chunk_overlap']}")
        print(f"  - top_k: {best['top_k']}")
        print(f"  - prompt_template: {best['prompt_template']}")
        
        print(f"\nğŸ’¡ ì¶”ì²œ ì‚¬í•­:")
        for rec in analysis['recommendations']:
            print(f"  - {rec}")
        
        print(f"\nğŸ“„ ìƒì„¸ ë¶„ì„: {output_path}")
    
    def _generate_recommendations(self, top_configs, all_results):
        """ìµœì  ì„¤ì • ì¶”ì²œ ìƒì„±"""
        recommendations = []
        
        if not top_configs:
            return ["ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        
        best = top_configs[0]
        
        # ì²­í¬ í¬ê¸° ì¶”ì²œ
        recommendations.append(
            f"ì²­í¬ í¬ê¸°ëŠ” {best['chunk_size']}ìê°€ ìµœì ì…ë‹ˆë‹¤."
        )
        
        # Top-K ì¶”ì²œ
        recommendations.append(
            f"ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ëŠ” {best['top_k']}ê°œê°€ ì í•©í•©ë‹ˆë‹¤."
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ì²œ
        recommendations.append(
            f"'{best['prompt_template']}' í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        )
        
        # ì†ë„ vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„
        if best['total_time'] > 10:
            recommendations.append(
                "ì‘ë‹µ ì‹œê°„ì´ ë‹¤ì†Œ ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•˜ë©´ top_kë¥¼ ì¤„ì´ì„¸ìš”."
            )
        
        return recommendations


def create_test_questions_template(output_path: str = "test_questions.json"):
    """í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ í…œí”Œë¦¿ ìƒì„±"""
    template = {
        "questions": [
            {
                "question": "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "category": "summary",
                "expected_topics": ["ë¬¸ì„œ ê°œìš”", "í•µì‹¬ ë‚´ìš©"]
            },
            {
                "question": "ì‹ ì²­ ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "category": "process",
                "expected_topics": ["ì ˆì°¨", "ë‹¨ê³„"]
            }
        ]
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ í…œí”Œë¦¿ ìƒì„±: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="RAG íŒŒë¼ë¯¸í„° íŠœë‹ ë„êµ¬")
    parser.add_argument("--model", type=str, 
                       default="mlx-community/Llama-3.2-1B-Instruct-4bit",
                       help="LLM ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--adapter", type=str, 
                       default="models/lora-adapter",
                       help="LoRA ì–´ëŒ‘í„° ê²½ë¡œ")
    parser.add_argument("--test-questions", type=str,
                       help="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ JSON íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", type=str, 
                       default="experiments",
                       help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--create-template", action="store_true",
                       help="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ í…œí”Œë¦¿ ìƒì„±")
    parser.add_argument("--quick", action="store_true",
                       help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (íŒŒë¼ë¯¸í„° ì¡°í•© ì¶•ì†Œ)")
    
    args = parser.parse_args()
    
    if args.create_template:
        create_test_questions_template()
        return
    
    # íŠœë„ˆ ì´ˆê¸°í™”
    tuner = RAGTuner(
        model_path=args.model,
        adapter_path=args.adapter,
        test_data_path=args.test_questions
    )
    
    # ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì„¤ì •
    if args.quick:
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ì¶•ì†Œ íŒŒë¼ë¯¸í„°
        chunk_sizes = [500]
        chunk_overlaps = [50]
        top_ks = [2, 3]
        prompt_templates = ["default", "korean_optimized"]
    else:
        # ì „ì²´ í…ŒìŠ¤íŠ¸
        chunk_sizes = [300, 500, 800]
        chunk_overlaps = [50, 100]
        top_ks = [1, 3, 5]
        prompt_templates = ["default", "detailed", "korean_optimized", "concise"]
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = tuner.run_experiment(
        chunk_sizes=chunk_sizes,
        chunk_overlaps=chunk_overlaps,
        top_ks=top_ks,
        prompt_templates=prompt_templates,
        output_dir=args.output
    )
    
    print("\n" + "="*60)
    print("âœ… RAG íŠœë‹ ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    main()

