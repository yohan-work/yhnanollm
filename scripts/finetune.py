#!/usr/bin/env python3
"""
MLX LoRA Fine-tuning Script for yhnanollm v1.0.0-beta
Apple Silicon (M3 Pro) optimized
"""

import argparse
import json
from pathlib import Path
from mlx_lm import load, lora


def prepare_data(data_path):
    """Alpaca í˜•ì‹ ë°ì´í„°ë¥¼ MLX í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Alpaca í˜•ì‹ì„ MLXê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    formatted_data = []
    for item in data:
        text = f"### Instruction:\n{item['instruction']}\n"
        if item.get('input'):
            text += f"### Input:\n{item['input']}\n"
        text += f"### Response:\n{item['output']}"
        formatted_data.append({"text": text})
    
    return formatted_data


def main():
    parser = argparse.ArgumentParser(description="MLX LoRA Fine-tuning")
    parser.add_argument("--model", type=str, default="mlx-community/Llama-3.2-1B-Instruct-4bit",
                        help="Base model path or HF repo (í† í° ë¶ˆí•„ìš”í•œ ê³µê°œ ëª¨ë¸)")
    parser.add_argument("--data", type=str, default="data/data-mini.json",
                        help="Training data path (Alpaca format)")
    parser.add_argument("--output", type=str, default="models/lora-adapter",
                        help="Output directory for LoRA weights")
    parser.add_argument("--iters", type=int, default=100,
                        help="Number of training iterations")
    parser.add_argument("--learning-rate", type=float, default=1e-5,
                        help="Learning rate")
    parser.add_argument("--lora-rank", type=int, default=8,
                        help="LoRA rank")
    parser.add_argument("--batch-size", type=int, default=2,
                        help="Batch size")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ yhnanollm v1.0.0-beta Fine-tuning ì‹œì‘")
    print(f"ğŸ“¦ ëª¨ë¸: {args.model}")
    print(f"ğŸ“Š ë°ì´í„°: {args.data}")
    print(f"ğŸ’¾ ì¶œë ¥: {args.output}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(args.output).mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„° ì¤€ë¹„
    print("\nğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
    train_data = prepare_data(args.data)
    print(f"âœ… {len(train_data)}ê°œì˜ í•™ìŠµ ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ")
    
    # MLX LoRA íŒŒì¸íŠœë‹
    # ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„œëŠ” mlx-lmì˜ lora.pyë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ê±°ë‚˜
    # ëª…ë ¹ì¤„ì—ì„œ mlx_lm.loraë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    print("\nâš™ï¸  LoRA í•™ìŠµ íŒŒë¼ë¯¸í„°:")
    print(f"  - Iterations: {args.iters}")
    print(f"  - Learning Rate: {args.learning_rate}")
    print(f"  - LoRA Rank: {args.lora_rank}")
    print(f"  - Batch Size: {args.batch_size}")
    
    # ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (MLX-LMì´ ìš”êµ¬í•˜ëŠ” í˜•ì‹)
    temp_data_path = Path(args.output) / "train.jsonl"
    with open(temp_data_path, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"\nğŸ’¡ MLX-LM CLIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤:")
    print(f"   mlx_lm.lora \\")
    print(f"     --model {args.model} \\")
    print(f"     --train \\")
    print(f"     --data {temp_data_path} \\")
    print(f"     --iters {args.iters} \\")
    print(f"     --learning-rate {args.learning_rate} \\")
    print(f"     --batch-size {args.batch_size} \\")
    print(f"     --adapter-path {args.output}")
    print(f"\n   # LoRA rankëŠ” configë¡œ ì„¤ì •í•˜ê±°ë‚˜ ê¸°ë³¸ê°’(8) ì‚¬ìš©")
    
    print("\n" + "="*60)
    print("âš ï¸  ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ ìœ„ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")
    print("="*60)


if __name__ == "__main__":
    main()

