# yhnanollm v1.0.0-beta

MLX κΈ°λ° ν•κµ­μ–΄ νμΈνλ‹
Apple Silicon (M3 Pro) μµμ ν™”λ TinyLlama λ¨λΈ LoRA νμΈνλ‹

## ν”„λ΅μ νΈ κ°μ”

- **λ² μ΄μ¤ λ¨λΈ**: `mlx-community/Llama-3.2-1B-Instruct-4bit`
- **λ°©λ²•λ΅ **: LoRA (Low-Rank Adaptation)
- **λ°μ΄ν„° ν•μ‹**: Alpaca JSON
- **λ©ν‘**: μ§§μ€ ν•κµ­μ–΄ μ‘λ‹µ νμΈνλ‹ ν…μ¤νΈ

## ν”„λ΅μ νΈ κµ¬μ΅°

```
yhnanollm/
β”β”€β”€ data/
β”‚   β””β”€β”€ data-mini.json          # μƒν” ν•κµ­μ–΄ ν•™μµ λ°μ΄ν„° (35κ° μμ‹)
β”β”€β”€ models/
β”‚   β””β”€β”€ lora-adapter/           # LoRA μ–΄λ‘ν„° κ°€μ¤‘μΉ (ν•™μµ ν›„)
β”β”€β”€ scripts/
β”‚   β”β”€β”€ finetune.py             # LoRA νμΈνλ‹ μ¤ν¬λ¦½νΈ
β”‚   β””β”€β”€ merge_and_test.py       # μ–΄λ‘ν„° λ³‘ν•© λ° ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β”β”€β”€ chat.py                     # λ€ν™”ν• μ±„ν… μΈν„°νμ΄μ¤
β”β”€β”€ requirements.txt            # Python μμ΅΄μ„±
β”β”€β”€ .gitignore
β””β”€β”€ README.md
```

## ν™κ²½ μ„¤μ •

```bash
# 1. κ°€μƒν™κ²½ μƒμ„±
python3 -m venv venv

# 2. κ°€μƒν™κ²½ ν™μ„±ν™”
source venv/bin/activate

# 3. μμ΅΄μ„± μ„¤μΉ
pip install --upgrade pip
pip install -r requirements.txt
```

## λΉ λ¥Έ μ‹μ‘ (CLI)

ν•™μµμ΄ μ™„λ£λμ—λ‹¤λ©΄ λ°”λ΅ λ€ν™”λ¥Ό μ‹μ‘ν•μ„Έμ”!

```bash
# κ°€μƒν™κ²½ ν™μ„±ν™”
source venv/bin/activate

# μ±„ν… μ‹μ‘!
python chat.py
```

**μ‚¬μ© μμ‹**:

```
π’¬ You: μ•λ…•ν•μ„Έμ”?
π¤– Bot: μ•λ…•ν•μ„Έμ”! λ¬΄μ—‡μ„ λ„μ™€λ“λ¦΄κΉμ”?

π’¬ You: Reactκ°€ λ­μ•Ό?
π¤– Bot: Reactλ” μ‚¬μ©μ μΈν„°νμ΄μ¤λ¥Ό λ§λ“¤κΈ° μ„ν• JavaScript λΌμ΄λΈλ¬λ¦¬μ…λ‹λ‹¤.
```

## π“– μ²μλ¶€ν„° ν•™μµν•κΈ°

### Step 1: λ°μ΄ν„° μ¤€λΉ„

μƒν” λ°μ΄ν„°κ°€ μ΄λ―Έ `data/data-mini.json`μ— μ¤€λΉ„λμ–΄ μμµλ‹λ‹¤.

**λ°μ΄ν„° ν•μ‹ (Alpaca JSON)**:

```json
[
  {
    "instruction": "μ§λ¬Έ λλ” λ…λ Ή",
    "input": "μ¶”κ°€ μ…λ ¥ (μ„ νƒμ‚¬ν•­)",
    "output": "μ›ν•λ” μ‘λ‹µ"
  }
]
```

### Step 2: LoRA νμΈνλ‹

```bash
# 1λ‹¨κ³„: λ°μ΄ν„° μ¤€λΉ„
python scripts/finetune.py

# 2λ‹¨κ³„: μ‹¤μ  ν•™μµ μ‹¤ν–‰ (ν† ν° λ¶ν•„μ”!)
mlx_lm.lora \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --train \
  --data data \
  --iters 100 \
  --learning-rate 1e-5 \
  --batch-size 2 \
  --adapter-path models/lora-adapter
```

ν•™μµμ€ μ•½ 2-5λ¶„ μ†μ”λ©λ‹λ‹¤. μ™„λ£λλ©΄ `models/lora-adapter/`μ— μ–΄λ‘ν„° νμΌμ΄ μƒμ„±λ©λ‹λ‹¤.

### Step 3: λ€ν™”ν• μ±„ν… μ‹μ‘

```bash
# ν•™μµλ λ¨λΈκ³Ό λ€ν™”ν•κΈ°
python chat.py

# μµμ… μ§€μ •
python chat.py --max-tokens 150

# λ² μ΄μ¤ λ¨λΈλ§ μ‚¬μ© (μ–΄λ‘ν„° μ—†μ΄)
python chat.py --no-adapter
```

**μ±„ν… λ…λ Ήμ–΄**:

- `exit`, `quit`, `q` - μΆ…λ£
- `history`, `h` - λ€ν™” νμ¤ν† λ¦¬ λ³΄κΈ°
- `clear`, `c` - νμ¤ν† λ¦¬ μ΄κΈ°ν™”
- `help`, `?` - λ„μ›€λ§

### Step 4: LoRA μ–΄λ‘ν„° λ³‘ν•© (μ„ νƒμ‚¬ν•­)

```bash
# LoRA μ–΄λ‘ν„°λ¥Ό λ² μ΄μ¤ λ¨λΈκ³Ό λ³‘ν•©
mlx_lm.fuse \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --adapter-path models/lora-adapter \
  --save-path models/merged-model
```

### Step 5: μ¤ν¬λ¦½νΈλ΅ ν…μ¤νΈ (μ„ νƒμ‚¬ν•­)

```bash
# LoRA μ–΄λ‘ν„°μ™€ ν•¨κ» ν…μ¤νΈ
python scripts/merge_and_test.py --test-only

# μ»¤μ¤ν…€ ν”„λ΅¬ν”„νΈλ΅ ν…μ¤νΈ
python scripts/merge_and_test.py --test-only --prompt "νμ΄μ¬μ΄λ€?"
```

### Step 6: MLX CLIλ΅ μ§μ ‘ μ‚¬μ© (κ³ κΈ‰)

```bash
# λ³‘ν•©λ λ¨λΈλ΅ μ§μ ‘ μ¶”λ΅ 
mlx_lm.generate \
  --model models/merged-model \
  --prompt "μ•λ…•ν•μ„Έμ”?" \
  --max-tokens 50

# LoRA μ–΄λ‘ν„°λ¥Ό μ‚¬μ©ν• μ¶”λ΅ 
mlx_lm.generate \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --adapter-path models/lora-adapter \
  --prompt "μ•λ…•ν•μ„Έμ”?" \
  --max-tokens 50
```

## ν•™μµ νλΌλ―Έν„°

| νλΌλ―Έν„°          | κΈ°λ³Έκ°’ | μ„¤λ…                        |
| ----------------- | ------ | --------------------------- |
| `--iters`         | 100    | ν•™μµ λ°λ³µ νμ              |
| `--learning-rate` | 1e-5   | ν•™μµλ¥                       |
| `--lora-rank`     | 8      | LoRA λ­ν¬ (λ‚®μ„μλ΅ κ°€λ²Όμ›€) |
| `--batch-size`    | 2      | λ°°μΉ ν¬κΈ°                   |

## ν & νΈλ¬λΈ”μν…

### M3 Pro μµμ ν™”

- MLXλ” Apple Siliconμ— μµμ ν™”λμ–΄ μμ–΄ GPU κ°€μ†μ„ μλ™μΌλ΅ ν™μ©ν•©λ‹λ‹¤
- λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•λ©΄ `--batch-size`λ¥Ό 1λ΅ λ‚®μ¶”μ„Έμ”
- LoRA rankλ¥Ό 4λ΅ λ‚®μ¶”λ©΄ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ μ¤„μ–΄λ“­λ‹λ‹¤

### λ°μ΄ν„° μ¶”κ°€

λ” λ§μ€ ν•™μµ λ°μ΄ν„°λ¥Ό μ¶”κ°€ν•λ ¤λ©΄ `data/data-mini.json`μ— Alpaca ν•μ‹μΌλ΅ μ¶”κ°€ν•μ„Έμ”.

### ν•™μµ μ‹κ°„

- 35κ° μƒν”, 200 iterations: μ•½ 4-5λ¶„ (M3 Pro κΈ°μ¤€)
- λ” λ§μ€ λ°μ΄ν„°μ™€ iterationsλ¥Ό μ‚¬μ©ν•  κ²½μ° μ‹κ°„μ΄ μ¦κ°€ν•©λ‹λ‹¤
