#!/usr/bin/env python3
"""
yhnanollm v1.0.0-beta - CLI ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
ë¡œì»¬ LLMê³¼ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ì±„íŒ… í”„ë¡œê·¸ë¨
"""

import argparse
import sys
from pathlib import Path
from mlx_lm import load, stream_generate
from mlx_lm.sample_utils import make_sampler, make_logits_processors


class LocalLLMChat:
    def __init__(self, model_path, adapter_path=None, max_tokens=100, temperature=0.3, repetition_penalty=1.1, top_p=0.9):
        self.model_path = model_path
        self.adapter_path = adapter_path
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.repetition_penalty = repetition_penalty
        self.top_p = top_p
        self.model = None
        self.tokenizer = None
        self.history = []
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        if self.adapter_path:
            print(f"   ë² ì´ìŠ¤ ëª¨ë¸: {self.model_path}")
            print(f"   LoRA ì–´ëŒ‘í„°: {self.adapter_path}")
            self.model, self.tokenizer = load(
                self.model_path,
                adapter_path=self.adapter_path
            )
        else:
            print(f"   ëª¨ë¸: {self.model_path}")
            self.model, self.tokenizer = load(self.model_path)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n")
    
    def format_prompt(self, user_input):
        """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        return f"### Instruction:\n{user_input}\n\në°˜ë“œì‹œ ìˆœìˆ˜ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n### Response:"
    
    def chat(self, user_input, skip_format=False):
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ì´ë¯¸ í¬ë§·ëœ í”„ë¡¬í”„íŠ¸
            skip_format: Trueë©´ í¬ë§·íŒ…ì„ ê±´ë„ˆë›°ê³  user_inputì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        """
        # RAGì—ì„œ ì´ë¯¸ í¬ë§·ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì€ ê²½ìš° í¬ë§·íŒ… ê±´ë„ˆë›°ê¸°
        if skip_format:
            prompt = user_input
        else:
            prompt = self.format_prompt(user_input)
        
        try:
            # samplerì™€ logits_processors ìƒì„±
            sampler = make_sampler(temp=self.temperature, top_p=self.top_p)
            logits_processors = make_logits_processors(repetition_penalty=self.repetition_penalty)
            
            # stream_generate ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            response_parts = []
            for response_obj in stream_generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=self.max_tokens,
                sampler=sampler,
                logits_processors=logits_processors
            ):
                response_parts.append(response_obj.text)
            
            response = "".join(response_parts)
            
            # íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.history.append({"user": user_input, "assistant": response})
            
            return response
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"âŒ LLM ìƒì„± ì˜¤ë¥˜:\n{error_detail}")
            return f"âŒ ì˜¤ë¥˜: {e}"
    
    def show_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥"""
        if not self.history:
            print("ì•„ì§ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*60)
        print("ğŸ“œ ëŒ€í™” íˆìŠ¤í† ë¦¬")
        print("="*60)
        for i, conv in enumerate(self.history, 1):
            print(f"\n[{i}] ì‚¬ìš©ì: {conv['user']}")
            print(f"    ì–´ì‹œìŠ¤í„´íŠ¸: {conv['assistant']}")
        print("="*60 + "\n")
    
    def clear_history(self):
        """íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.history = []
        print("âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
    
    def print_help(self):
        """ë„ì›€ë§ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“– ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
        print("="*60)
        print("  exit, quit, q     : í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print("  history, h        : ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³´ê¸°")
        print("  clear, c          : íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")
        print("  help, ?           : ë„ì›€ë§ ë³´ê¸°")
        print("="*60 + "\n")
    
    def run(self):
        """ë©”ì¸ ëŒ€í™” ë£¨í”„"""
        # ëª¨ë¸ ë¡œë“œ
        self.load_model()
        
        # í™˜ì˜ ë©”ì‹œì§€
        print("="*60)
        print("ğŸ¤– yhnanollm v1.0.0-beta - ë¡œì»¬ LLM ì±„íŒ…")
        print("="*60)
        print("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! (ì¢…ë£Œ: 'exit', ë„ì›€ë§: 'help')\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
                user_input = input("\nğŸ’¬ You: ").strip()
                
                # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
                if not user_input:
                    continue
                
                # ëª…ë ¹ì–´ ì²˜ë¦¬
                if user_input.lower() in ['exit', 'quit', 'q']:
                    print("\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                    break
                
                elif user_input.lower() in ['history', 'h']:
                    self.show_history()
                    continue
                
                elif user_input.lower() in ['clear', 'c']:
                    self.clear_history()
                    continue
                
                elif user_input.lower() in ['help', '?']:
                    self.print_help()
                    continue
                
                # ì‘ë‹µ ìƒì„±
                print("ğŸ¤– Bot: ", end="", flush=True)
                response = self.chat(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue


def main():
    parser = argparse.ArgumentParser(
        description="yhnanollm CLI ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="mlx-community/Llama-3.2-1B-Instruct-4bit",
        help="ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace repo"
    )
    parser.add_argument(
        "--adapter",
        type=str,
        default="models/lora-adapter",
        help="LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=100,
        help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜"
    )
    parser.add_argument(
        "--no-adapter",
        action="store_true",
        help="ì–´ëŒ‘í„° ì—†ì´ ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©"
    )
    
    args = parser.parse_args()
    
    # ì–´ëŒ‘í„° ê²½ë¡œ í™•ì¸
    adapter_path = None if args.no_adapter else args.adapter
    
    # ì–´ëŒ‘í„°ê°€ ì—†ìœ¼ë©´ ê²½ê³ 
    if adapter_path and not Path(adapter_path).exists():
        print(f"âš ï¸  ê²½ê³ : ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_path}")
        print("   ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
        adapter_path = None
    
    # ì±„íŒ… ì‹œì‘
    chat = LocalLLMChat(
        model_path=args.model,
        adapter_path=adapter_path,
        max_tokens=args.max_tokens
    )
    
    chat.run()


if __name__ == "__main__":
    main()

